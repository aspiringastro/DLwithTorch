{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPETmq/IDh3sKfVTNPFBogy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "510c618ae7fa4a3781e6b60847af0a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e81ca6d96727492498d2a26f5b0655f3",
              "IPY_MODEL_15ff9a3cb87e4f59a83ab327183209f2",
              "IPY_MODEL_d0d9a49a715f4688b5f8708f6155e25c"
            ],
            "layout": "IPY_MODEL_5940689e371e4f049afd94d1e389c449"
          }
        },
        "e81ca6d96727492498d2a26f5b0655f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4cfce7421c14f35980c262491076d83",
            "placeholder": "​",
            "style": "IPY_MODEL_feccb84f2c8a4ac88490a51a1029db90",
            "value": "100%"
          }
        },
        "15ff9a3cb87e4f59a83ab327183209f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cb7a385bfe647beb5e41be71ea7b584",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a9396d3c31840cfab15c1cdf6370dc1",
            "value": 170498071
          }
        },
        "d0d9a49a715f4688b5f8708f6155e25c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a283f56fb11d43668db8a71c678135d6",
            "placeholder": "​",
            "style": "IPY_MODEL_b69a0bd8675b4dc4aff901a10ff8f0d5",
            "value": " 170498071/170498071 [00:02&lt;00:00, 96425975.94it/s]"
          }
        },
        "5940689e371e4f049afd94d1e389c449": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4cfce7421c14f35980c262491076d83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "feccb84f2c8a4ac88490a51a1029db90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cb7a385bfe647beb5e41be71ea7b584": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a9396d3c31840cfab15c1cdf6370dc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a283f56fb11d43668db8a71c678135d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b69a0bd8675b4dc4aff901a10ff8f0d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aspiringastro/DLwithTorch/blob/main/vit_version_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7winnVlZ7Rfw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms as T\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "\n",
        "class CIFAR10DataSet():\n",
        "    def __init__(self, data_dir=\"data/cifar10\", train_val_split=0.8):\n",
        "        self.data_dir = data_dir\n",
        "        self.dataset = CIFAR10(root=self.data_dir, download=True)\n",
        "        self.mean = (0.485, 0.456, 0.406)\n",
        "        self.std = (0.229, 0.224, 0.225)\n",
        "        self.train_val_split = train_val_split\n",
        "\n",
        "    def  train_dataloader(self, batch_size=32, resize=32, p=0.5, mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225), num_workers=4):\n",
        "        tf = T.Compose([\n",
        "                T.RandomResizedCrop(size=resize),\n",
        "                T.RandomHorizontalFlip(p=p),\n",
        "                T.RandomVerticalFlip(p=p),\n",
        "                T.ToTensor(),\n",
        "                T.Normalize(mean, std),\n",
        "            ]\n",
        "        )\n",
        "        ds = CIFAR10(root=self.data_dir, train=True, transform=tf)\n",
        "        num_train = len(ds)\n",
        "        indices = list(range(num_train))\n",
        "        split = int(np.floor(self.train_val_split * num_train))\n",
        "        train_sampler = SubsetRandomSampler(indices[split:])\n",
        "        dl = DataLoader(\n",
        "            ds,\n",
        "            batch_size=batch_size, \n",
        "            num_workers=num_workers, \n",
        "            sampler=train_sampler, \n",
        "            drop_last=True\n",
        "            )\n",
        "        return dl\n",
        "    \n",
        "    def  val_dataloader(self, batch_size=32, resize=32, p=0.5, mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225), num_workers=4):\n",
        "        tf = T.Compose([\n",
        "                T.RandomResizedCrop(size=resize),\n",
        "                T.RandomHorizontalFlip(p=p),\n",
        "                T.RandomVerticalFlip(p=p),\n",
        "                T.ToTensor(),\n",
        "                T.Normalize(mean, std),\n",
        "            ]\n",
        "        )\n",
        "        ds = CIFAR10(root=self.data_dir, train=True, transform=tf)\n",
        "        num_train = len(ds)\n",
        "        indices = list(range(num_train))\n",
        "        split = int(np.floor(self.train_val_split * num_train))\n",
        "        val_sampler = SubsetRandomSampler(indices[:split])\n",
        "        dl = DataLoader(\n",
        "            ds,\n",
        "            batch_size=batch_size, \n",
        "            num_workers=num_workers, \n",
        "            sampler=val_sampler, \n",
        "            drop_last=True\n",
        "            )\n",
        "        return dl\n",
        "\n",
        "    \n",
        "    def test_dataloader(self, batch_size=32, mean=(0.485, 0.456, 0.406) ,std=(0.229, 0.224, 0.225), num_workers=2):\n",
        "        tf = T.Compose([\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean, std),\n",
        "            ]\n",
        "        )\n",
        "        ds = CIFAR10(root=self.data_dir, train=False, transform=tf)\n",
        "        dl = DataLoader(ds,batch_size=batch_size, num_workers=num_workers, drop_last=True)\n",
        "        return dl\n",
        "    \n",
        "    def get_next(self, dl):\n",
        "        return next(iter(dl))\n",
        "    \n",
        "    def get_classes(self):\n",
        "        classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "        return classes\n",
        "\n"
      ],
      "metadata": {
        "id": "jyE-1BRS7XHa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.utils\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
        "\n",
        "def make_image(img, mean=(0., 0., 0.), std=(1., 1., 1.)):\n",
        "    #denormalize\n",
        "    for i in range(3):\n",
        "        img[i] = img[i] * std[i] + mean[i]\n",
        "    npimg = img.numpy()\n",
        "    return np.transpose(npimg, (1, 2, 0))\n",
        "\n",
        "def show_image(imgs, mean=(0., 0., 0.), std=(1., 1., 1.)):\n",
        "    if not isinstance(imgs, list):\n",
        "        imgs = [imgs]\n",
        "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
        "    for i, img in enumerate(imgs):\n",
        "        for j in range(3):\n",
        "            img[j] = img[j] * std[j] + mean[j]\n",
        "        img = img.detach()\n",
        "        img = F.to_pil_image(img)\n",
        "        axs[0, i].imshow(np.asarray(img))\n",
        "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "def show_images(imgs, mean=(0., 0., 0.), std=(1., 1., 1.)):\n",
        "    grid_imgs = make_grid(imgs)\n",
        "    grid_imgs = make_image(grid_imgs, mean, std)\n",
        "    plt.imshow(grid_imgs)\n",
        "    plt.axis('off')\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "rbich_FF7aRJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "eval_iters = 200\n",
        "dropout = 0.2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "DkVgyrqq7cZq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CIFAR10DataSet()\n",
        "downloader = dataset.train_dataloader()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "4KvJ8QfxADTl",
        "outputId": "80666f2c-177e-4ef8-e406-6f0c715c088e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7efbcfab01c0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<enumerate object at 0x7efbcf9367c0>\n",
            "0 torch.Size([32, 3, 32, 32]) torch.Size([32])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f4b6cf6bcbfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mshow_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%5s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-ac2ebe765861>\u001b[0m in \u001b[0;36mshow_images\u001b[0;34m(imgs, mean, std)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mgrid_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mgrid_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-ac2ebe765861>\u001b[0m in \u001b[0;36mmake_image\u001b[0;34m(img, mean, std)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnpimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, axes)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \"\"\"\n\u001b[0;32m--> 660\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transpose'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: axes don't match array"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(dataset, split, batch_size=32):\n",
        "    if split == 'train':\n",
        "        downloader = dataset.train_dataloader(batch_size=batch_size)\n",
        "    elif split == 'val':\n",
        "        downloader = dataset.val_dataloader(batch_size=batch_size)\n",
        "    elif split == 'test':\n",
        "        downloader = dataset.test_dataloader(batch_size=batch_size)\n",
        "    else:\n",
        "        raise AttributeError(f'Invalid Split parameter ({split}) provided.')\n",
        "    x, y_label = dataset.get_next(downloader)\n",
        "    y = F.one_hot(y_label, num_classes=len(dataset.get_classes()))\n",
        "    x, y = x.to(device), y.float().to(device)\n",
        "    return x,y\n",
        "\n",
        "dataset = CIFAR10DataSet()\n",
        "\n",
        "x, y = get_batch(dataset, \"train\")\n",
        "print(x.shape, y.shape)\n",
        "\n",
        "x_val, y_val = get_batch(dataset, \"val\")\n",
        "print(x_val.shape, y_val.shape)\n",
        "\n",
        "x_test, y_test= get_batch(dataset, \"test\")\n",
        "print(x_test.shape, y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "510c618ae7fa4a3781e6b60847af0a73",
            "e81ca6d96727492498d2a26f5b0655f3",
            "15ff9a3cb87e4f59a83ab327183209f2",
            "d0d9a49a715f4688b5f8708f6155e25c",
            "5940689e371e4f049afd94d1e389c449",
            "a4cfce7421c14f35980c262491076d83",
            "feccb84f2c8a4ac88490a51a1029db90",
            "6cb7a385bfe647beb5e41be71ea7b584",
            "7a9396d3c31840cfab15c1cdf6370dc1",
            "a283f56fb11d43668db8a71c678135d6",
            "b69a0bd8675b4dc4aff901a10ff8f0d5"
          ]
        },
        "id": "1YkG_g0p7jtg",
        "outputId": "f89839f8-0abc-424e-bcc3-c7e90b43a735"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "510c618ae7fa4a3781e6b60847af0a73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar10/cifar-10-python.tar.gz to data/cifar10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 32, 32]) torch.Size([32, 10])\n",
            "torch.Size([32, 3, 32, 32]) torch.Size([32, 10])\n",
            "torch.Size([32, 3, 32, 32]) torch.Size([32, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Patch embed layer that takes a 2D image to create embed patches of size P\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size, patch_size, in_chans=3, embed_dim=96):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "        self.patch_embd = nn.Conv2d(\n",
        "            in_chans,\n",
        "            embed_dim,\n",
        "            kernel_size=patch_size, \n",
        "            stride=patch_size\n",
        "            )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # print(f\"PatchEmbedding: x.shape: {x.shape}\")\n",
        "        x = self.patch_embd(x)\n",
        "        # print(f\"PatchEmbedding: patch_embd(x).shape: {x.shape}\")\n",
        "        x = x.flatten(2)\n",
        "        # print(f\"PatchEmbedding: flatten(patch_embed(x)).shape: {x.shape}\")\n",
        "        x = x.transpose(1,2)\n",
        "        # print(f\"PatchEmbedding: transpose(flatten(patch_embed(x)),(1,2)).shape: {x.shape}\")\n",
        "        return x"
      ],
      "metadata": {
        "id": "YBxiT17-7jO4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(dataset, split, batch_size=32):\n",
        "    if split == 'train':\n",
        "        downloader = dataset.train_dataloader(batch_size=batch_size)\n",
        "    elif split == 'val':\n",
        "        downloader = dataset.val_dataloader(batch_size=batch_size)\n",
        "    elif split == 'test':\n",
        "        downloader = dataset.test_dataloader(batch_size=batch_size)\n",
        "    else:\n",
        "        raise AttributeError(f'Invalid Split parameter ({split}) provided.')\n",
        "    x, y_label = dataset.get_next(downloader)\n",
        "    y = F.one_hot(y_label, num_classes=len(dataset.get_classes()))\n",
        "    return x,y.float()\n",
        "\n",
        "x, y = get_batch(dataset, \"train\")\n",
        "print(x.shape, y.shape)\n",
        "\n",
        "x_val, y_val = get_batch(dataset, \"val\")\n",
        "print(x_val.shape, y_val.shape)\n",
        "\n",
        "x_test, y_test= get_batch(dataset, \"test\")\n",
        "print(x_test.shape, y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66X8t0AM7l7B",
        "outputId": "ed4aec39-9228-4637-cd5f-a8bbe36b9e8b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 32, 32]) torch.Size([32, 10])\n",
            "torch.Size([32, 3, 32, 32]) torch.Size([32, 10])\n",
            "torch.Size([32, 3, 32, 32]) torch.Size([32, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of single attention \"\"\"\n",
        "    def __init__(self, head_size, n_embd):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)  # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5 # sqrt of head size, (B,T,C) @ (B,T,C)^T => (B,T,C) @ (B,C,T) => (B,T,T)\n",
        "        wei = F.softmax(wei, dim=1) # (B,T,T)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x) #(B,T,C)\n",
        "        out = wei @ v # (B,T,T) @ (B,T,C) = (B,T,C)\n",
        "        return out"
      ],
      "metadata": {
        "id": "rueKHPJf730V"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size, n_embd):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size, n_embd) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # concat over channel dimension\n",
        "        out = self.proj(out) # projection is a linear transformation of the outcome of the previous multi-head layer\n",
        "        out = self.dropout(out) # dropout\n",
        "        return out"
      ],
      "metadata": {
        "id": "sVLF1dEP8ZwN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer of feedforward followed by non-linearity\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # projection layer in FFwd\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.nn(x)"
      ],
      "metadata": {
        "id": "PoxA5li98bqP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer Block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd : embedding dimension\n",
        "        # n_head : number of heads needed for multi-head self-attention\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0, f'n_embd {n_embd}, n_head: {n_head} must be a divisor'\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size, n_embd) # communication\n",
        "        self.ffwd = FeedForward(n_embd) # computation\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # No residual connections\n",
        "        # x = self.sa(x)\n",
        "        # x = self.ffwd(x)\n",
        "        # with residual connection\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "2W7lbSIf8ddB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=96, n_classes=10, n_layers=4, n_heads=6):\n",
        "        super().__init__()\n",
        "        # Every patch sequence begins with a CLS token\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "\n",
        "        self.patch_embedding_table = PatchEmbedding(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        self.position_embedding_table = nn.Embedding(self.patch_embedding_table.n_patches, embedding_dim=embed_dim)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[ Block(embed_dim, n_heads) for _ in range(n_layers)],\n",
        "            nn.LayerNorm(embed_dim),\n",
        "        )\n",
        "        self.vm_head = nn.Linear(embed_dim, n_classes)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        n_samples, n_chans, n_patch, _ = idx.shape\n",
        "        # print(f\"VisionTransformerModel: n_samples={n_samples}, n_chans={n_chans}, n_patch={n_patch}\")\n",
        "        \n",
        "        patch_emb = self.patch_embedding_table(idx)\n",
        "        # print(f\"VisionTransformerModel: patch_emb shape={patch_emb.shape}\")\n",
        "        n_patches = self.patch_embedding_table.n_patches\n",
        "        pos_emb = self.position_embedding_table(torch.arange(n_patches, device=device))\n",
        "        # print(f\"VisionTransformerModel: pos_emb shape={pos_emb.shape}\")\n",
        "        x = patch_emb + pos_emb\n",
        "        # print(f\"VisionTransformerModel: x (after patch+pos) shape={x.shape}\")\n",
        "\n",
        "        # Prepend the cls_token\n",
        "        cls_token = self.cls_token.expand(n_samples, -1, -1)\n",
        "        # print(f\"VisionTransformerModel: cls_token shape={cls_token.shape}\")\n",
        "        x =  torch.cat((cls_token, x), dim=1)\n",
        "        # print(f\"VisionTransformerModel: x (after cat cls_token) shape={x.shape}\")\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        # print(f\"VisionTransformerModel: x.blocks shape={x.shape}\")\n",
        "\n",
        "        cls_token_final = x[:, 0]\n",
        "        # print(f\"VisionTransformerModel: cls_token_final shape={cls_token_final.shape}\")\n",
        "\n",
        "        logits = self.softmax(self.vm_head(cls_token_final))\n",
        "        \n",
        "        # print(f\"VisionTransformerModel: logits shape={logits.shape}\")\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # B, T, C = logits.shape\n",
        "            # print(f\"VisionTransformerModel: B T C shape={B} {T} {C}\")\n",
        "            # logits = logits.view(B*T, C)\n",
        "            # targets = targets.view(B*T)\n",
        "            # print(f\"VisionTransformerModel: Logits = {logits.shape}\\n{logits}\\n\")\n",
        "            # print(f\"VisionTransformerModel: Targets = {targets.shape}\\n{targets}\\n\")\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "    \n"
      ],
      "metadata": {
        "id": "nh53L8el8fe1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformerModel()\n",
        "m = model.to(device)"
      ],
      "metadata": {
        "id": "2RT1oZGu8kCq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(dataset, split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "OmnQhDPK8kec"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3Hy2Zkt8mvv",
        "outputId": "ce5318bd-fa1b-4ce8-95ed-fb7d72dc3b7d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.458314 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 3e-3\n",
        "max_iters = 1000\n",
        "eval_interval = 100\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "from tqdm import tqdm\n",
        "\n",
        "for iter in tqdm(range(max_iters)):\n",
        "    \n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch(dataset, 'train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "z3JtR59h8oV6",
        "outputId": "49ed6a22-8dbf-48ec-dc75-ca66c69bac02"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-6193a314077e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# every once in a while evaluate the loss on train and val sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-8d069f60728a>\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-e2900794900f>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(dataset, split, batch_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Invalid Split parameter ({split}) provided.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-f7ebdfa295a6>\u001b[0m in \u001b[0;36mget_next\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
          ]
        }
      ]
    }
  ]
}