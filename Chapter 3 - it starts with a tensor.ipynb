{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3603c1-9593-4584-bdc9-36481a1e4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae6af63-7b8a-4021-86d7-a5bd1b9ea6eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb9f8ce-ce51-46bf-b188-3b2cd56cc528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.), 1.0, 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1], a[1].item(), float(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f7fe37-1582-4961-a06a-97fdccdf1585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 2.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2] = 2.0\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b244ee92-10fe-4fc2-abc9-879f0768fa75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([float(a) for a in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2f2e35f-9fe2-48cc-ba15-035b3e726e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10).type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f53075-55dc-46c4-99a4-98882c4cdbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.tensor([[x, y] for x in range(3) for y in range(3)]).type(torch.FloatTensor)\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd529b88-7232-4819-8c16-25d6ebe1bf3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4049832c-3108-4cad-a5be-df4d6f86ea71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zp = torch.zeros(points.shape)\n",
    "zp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0d046f7-63a6-48c3-8d73-63f2c0ac3484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(2.))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[2,0], points[2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d5daa3a-74cf-4c8e-8be4-e421334f545e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4023c561-d293-4528-affd-9c7454bc54d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 0., 2., 1., 0., 1.],\n",
       "        [1., 1., 2., 2., 0., 2., 1., 2., 2.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.view(-1, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2290a25-3c39-4363-9c00-d0b66d36ecc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [0., 2.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 2.],\n",
       "        [2., 0.],\n",
       "        [2., 1.],\n",
       "        [2., 2.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a49639f4-0292-41c4-9c79-2dbd2a986a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 1., 1., 2., 2., 2.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a4f9f8b-da94-46a6-8f7f-180dc8040151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "unsqueeze(input, dim) -> Tensor\n",
       "\n",
       "Returns a new tensor with a dimension of size one inserted at the\n",
       "specified position.\n",
       "\n",
       "The returned tensor shares the same underlying data with this tensor.\n",
       "\n",
       "A :attr:`dim` value within the range ``[-input.dim() - 1, input.dim() + 1)``\n",
       "can be used. Negative :attr:`dim` will correspond to :meth:`unsqueeze`\n",
       "applied at :attr:`dim` = ``dim + input.dim() + 1``.\n",
       "\n",
       "Args:\n",
       "    input (Tensor): the input tensor.\n",
       "    dim (int): the index at which to insert the singleton dimension\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> x = torch.tensor([1, 2, 3, 4])\n",
       "    >>> torch.unsqueeze(x, 0)\n",
       "    tensor([[ 1,  2,  3,  4]])\n",
       "    >>> torch.unsqueeze(x, 1)\n",
       "    tensor([[ 1],\n",
       "            [ 2],\n",
       "            [ 3],\n",
       "            [ 4]])\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?torch.unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b33dd19-a3f6-45d7-9a0d-085a3711c7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Understanding squeeze and unsqueeze a tensor\n",
    "x = torch.tensor([1,2,3,4])\n",
    "x_u = torch.unsqueeze(x, 1)\n",
    "x_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d21a745d-58ed-48b6-bdcf-e6b6f437d6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 5]) torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1090, -0.7284,  0.5796, -0.4119,  0.6711],\n",
       "          [ 0.7876, -0.5574, -0.0578,  0.6180,  1.1130],\n",
       "          [-0.2739,  2.2963,  0.1230, -2.1484, -1.0580],\n",
       "          [-0.4162, -0.3342, -0.7634,  1.8820,  1.4100],\n",
       "          [ 0.3992,  1.8552,  1.0967, -0.2686, -0.4048]],\n",
       " \n",
       "         [[-0.4378, -0.8048,  0.8616, -1.5140,  1.3408],\n",
       "          [-0.9288, -0.7073,  1.1574, -0.3663, -0.6444],\n",
       "          [-0.6761,  1.1012, -1.4716,  1.8130,  1.3793],\n",
       "          [ 1.5779, -0.1228,  0.8835,  0.5812,  0.7009],\n",
       "          [-1.5709, -0.2410, -0.1468,  0.7028, -1.2589]],\n",
       " \n",
       "         [[-0.1122, -0.6114, -0.0785,  0.2500,  0.4444],\n",
       "          [-0.5568, -0.1312,  0.0203,  2.3416, -0.0651],\n",
       "          [-0.7808,  1.8602,  1.0304, -0.6279, -0.3822],\n",
       "          [-0.4769, -1.6649, -0.1132,  0.9057,  0.3448],\n",
       "          [-0.9538, -0.4781, -0.0276,  0.6238, -0.4336]]]),\n",
       " tensor([0.2126, 0.7152, 0.0722]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In[2]:\n",
    "img_t = torch.randn(3, 5, 5) # shape [channels, rows, columns]\n",
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])\n",
    "print(img_t.shape, weights.shape)\n",
    "img_t, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bc569640-6221-49f5-a6ce-42330aa22b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.1418, -1.0305, -0.9609, -0.6876,  1.6747],\n",
       "          [-0.3175,  1.6457,  0.9897, -0.4149, -0.5998],\n",
       "          [-0.0976, -1.8780,  0.0468,  1.9805,  0.4391],\n",
       "          [ 0.9731, -0.6426,  0.7858,  0.4878, -0.2792],\n",
       "          [-0.9837,  1.3873, -0.9406, -0.4178,  0.0659]],\n",
       "\n",
       "         [[-0.5803,  0.7585,  0.1412, -1.3239,  0.0895],\n",
       "          [ 0.0967,  0.1733, -1.2939,  0.0761, -0.3867],\n",
       "          [ 0.9663, -1.1469,  0.8991, -0.2901, -0.2066],\n",
       "          [ 0.2643, -0.4219,  0.6456, -1.7823, -0.2775],\n",
       "          [-0.8489,  0.7288,  1.0670, -0.7340,  0.5099]],\n",
       "\n",
       "         [[ 1.0408,  0.9069, -3.3066, -0.1595,  0.8415],\n",
       "          [ 0.3209, -0.0641,  0.3792, -1.5061, -0.8055],\n",
       "          [-1.2001, -1.0738, -1.1097,  0.3130, -0.8721],\n",
       "          [ 0.6300, -0.2924,  0.3002, -0.8188, -1.3120],\n",
       "          [-0.4202, -0.4668,  0.8105, -0.3109,  0.4952]]],\n",
       "\n",
       "\n",
       "        [[[-0.4297,  0.0511,  1.7481,  1.5133, -0.0131],\n",
       "          [ 0.2749,  0.2281, -0.0379,  0.1618,  0.6127],\n",
       "          [ 1.1308,  0.3467,  1.4320, -0.1875,  0.8465],\n",
       "          [ 0.8441, -0.8675,  1.5414, -1.9267, -0.1401],\n",
       "          [ 1.8583, -0.5409,  0.1033, -0.8520,  0.9565]],\n",
       "\n",
       "         [[ 0.7334,  1.5610,  0.8297, -0.5685, -0.2679],\n",
       "          [-0.9525,  0.3895,  1.8976,  0.2906, -0.4617],\n",
       "          [ 1.8350, -1.5657,  0.2999,  0.3525,  0.5063],\n",
       "          [-0.2991,  0.1518,  0.3558, -0.8506,  0.8608],\n",
       "          [ 0.1562, -0.7206,  0.5411, -0.2204,  0.0460]],\n",
       "\n",
       "         [[-0.7892,  0.7780,  0.6737, -1.3854,  0.5459],\n",
       "          [ 0.5842, -1.3142,  0.0059,  1.9419, -0.1812],\n",
       "          [ 1.2106, -1.1685,  0.3236, -0.1513,  1.0572],\n",
       "          [-2.4329,  1.6089, -0.9514, -1.5330, -0.9498],\n",
       "          [-0.3642,  0.7168,  0.7639, -0.0600,  0.9403]]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In[3]:\n",
    "batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns]\n",
    "print(batch_t.shape)\n",
    "batch_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b08b8f20-5341-4bf1-b409-c2f3c27b175d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5]) torch.Size([2, 5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2197, -0.7148,  0.4542, -0.5586,  0.8188],\n",
       "        [-0.2326, -0.4653,  0.3733,  0.8644,  0.1345],\n",
       "        [-0.5769,  1.7526, -0.1061, -0.3211, -0.0203],\n",
       "        [ 0.2282, -0.7073,  0.0023,  1.1230,  0.8186],\n",
       "        [-0.7085,  0.3787,  0.3074,  0.3527, -0.6991]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In[4]:\n",
    "img_gray_naive = img_t.mean(-3)\n",
    "batch_gray_naive = batch_t.mean(-3)\n",
    "print(img_gray_naive.shape, batch_gray_naive.shape)\n",
    "img_gray_naive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "976a0b92-d88b-4b84-a0b9-a8208594b643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 5, 5]), torch.Size([2, 3, 5, 5]), torch.Size([3, 1, 1]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In[5]:\n",
    "# \n",
    "# PyTorch will allow us to multiply things that are the same shape, as well as shapes where one operand is of size 1 in a given dimension. \n",
    "# It also appends leading dimensions of size 1 automatically. This is a feature called broadcasting. \n",
    "# batch_t of shape (2, 3, 5, 5) is multiplied by unsqueezed_weights of shape (3, 1, 1), resulting in a tensor of shape (2, 3, 5, 5), \n",
    "# from which we can then sum the third dimension from the end (the three channels):\n",
    "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze_(-1)\n",
    "img_weights = (img_t * unsqueezed_weights)\n",
    "batch_weights = (batch_t * unsqueezed_weights)\n",
    "img_gray_weighted = img_weights.sum(-3)\n",
    "batch_gray_weighted = batch_weights.sum(-3)\n",
    "batch_weights.shape, batch_t.shape, unsqueezed_weights.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "12d24330-326d-4c9b-b1a9-5620b50b08cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 5])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "img_gray_weighted_fancy = torch.einsum('...chw,c->...hw', img_t, weights)\n",
    "batch_gray_weighted_fancy = torch.einsum('...chw,c->...hw', batch_t, weights)\n",
    "batch_gray_weighted_fancy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c666c6ba-5696-45f6-91be-70249e2b4e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "\n",
    "weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels'])\n",
    "weights_named\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7a37cfa1-e1d8-4f24-be07-47475c9ea670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img named: torch.Size([3, 5, 5]) ('channels', 'rows', 'columns')\n",
      "batch named: torch.Size([2, 3, 5, 5]) (None, 'channels', 'rows', 'columns')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img_named =  img_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "batch_named = batch_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "print(\"img named:\", img_named.shape, img_named.names)\n",
    "print(\"batch named:\", batch_named.shape, batch_named.names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e1bc776b-2d47-4165-92ab-2ab4f2d2d028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 1]), ('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weights_aligned = weights_named.align_as(img_named)\n",
    "weights_aligned.shape, weights_aligned.names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cef96fc2-4150-4cc7-b8e8-2fd5c90c587e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), ('rows', 'columns'))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gray_named = (img_named * weights_aligned).sum('channels')\n",
    "gray_named.shape, gray_named.names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "de9b0b9d-9a62-492f-91c0-8af172896998",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gray_named \u001b[38;5;241m=\u001b[39m (\u001b[43mimg_named\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweights_named\u001b[49m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannels\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match."
     ]
    }
   ],
   "source": [
    "gray_named = (img_named[..., :3] * weights_named).sum('channels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3d1a7b6a-a9c0-4469-839e-565d6c6d9cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), (None, None))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In[12]:\n",
    "gray_plain = gray_named.rename(None)\n",
    "gray_plain.shape, gray_plain.names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fd8e952e-64a1-4f6b-922f-9bf5441a8191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float64, torch.int16)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Managing a tensor's dtype attribute\n",
    "\n",
    "double_points = torch.ones(10,2, dtype=torch.double)\n",
    "short_points = torch.tensor([[1,2], [3,4]], dtype=torch.short)\n",
    "double_points.dtype, short_points.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0087220c-9b07-4a09-a1e8-74e005e21751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.int32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "double_points = torch.zeros(10, 2).double()\n",
    "short_points = torch.ones(10, 2).short()\n",
    "double_points = torch.zeros(10, 2).to(torch.float32)\n",
    "short_points = torch.ones(10, 2).to(dtype=torch.int)\n",
    "double_points.dtype, short_points.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376bc204-689c-46da-abdc-63c6b9520c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
